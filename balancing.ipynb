{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Balancing, cv preprocessing and augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing import image\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/nbs/data/balanced_preprocessed_seedlings\"\n",
    "train_path = os.path.join(path, \"train\")\n",
    "valid_path = os.path.join(path, \"valid\")\n",
    "train_preproc_path = os.path.join(path,\"train_preproc\")\n",
    "valid_preproc_path = os.path.join(path,\"valid_preproc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset to train and valid (split koef 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_koef = 0.8\n",
    "cat_size = {}\n",
    "files_to_remove = []\n",
    "for cat in os.listdir(train_path):\n",
    "    valid_cat_dir = os.path.join(valid_path, cat)\n",
    "    train_cat_dir = os.path.join(train_path, cat)\n",
    "    cat_size[cat] = len(os.listdir(train_cat_dir))\n",
    "    if not os.path.exists(valid_cat_dir):\n",
    "        os.mkdir(valid_cat_dir)\n",
    "    files = os.listdir(train_cat_dir)\n",
    "    x_files, y_files = train_test_split(files, test_size=1-split_koef)\n",
    "    for f in y_files:\n",
    "        from_file = os.path.join(train_cat_dir, f)\n",
    "        to_file = os.path.join(valid_cat_dir, f)\n",
    "        shutil.copy(from_file, to_file)\n",
    "        files_to_remove.append(from_file)\n",
    "        \n",
    "for f in files_to_remove:\n",
    "        os.remove(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at Train and Valid data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Black-grass               : 211\n",
      "Charlock                  : 313\n",
      "Cleavers                  : 230\n",
      "Common Chickweed          : 489\n",
      "Common wheat              : 177\n",
      "Fat Hen                   : 381\n",
      "Loose Silky-bent          : 524\n",
      "Maize                     : 177\n",
      "Scentless Mayweed         : 413\n",
      "Shepherds Purse           : 185\n",
      "Small-flowered Cranesbill : 397\n",
      "Sugar beet                : 309\n",
      "\n",
      "Valid set\n",
      "Black-grass               : 54\n",
      "Charlock                  : 79\n",
      "Cleavers                  : 59\n",
      "Common Chickweed          : 124\n",
      "Common wheat              : 46\n",
      "Fat Hen                   : 96\n",
      "Loose Silky-bent          : 132\n",
      "Maize                     : 46\n",
      "Scentless Mayweed         : 105\n",
      "Shepherds Purse           : 48\n",
      "Small-flowered Cranesbill : 101\n",
      "Sugar beet                : 129\n"
     ]
    }
   ],
   "source": [
    "train_cat_size = {}\n",
    "valid_cat_size = {}\n",
    "for cat in os.listdir(train_path):\n",
    "    train_cat_dir = os.path.join(train_path, cat)\n",
    "    train_cat_size[cat] = len(os.listdir(train_cat_dir))\n",
    "\n",
    "for cat in os.listdir(valid_path):\n",
    "    valid_cat_dir = os.path.join(valid_path, cat)\n",
    "    valid_cat_size[cat] = len(os.listdir(valid_cat_dir))\n",
    "\n",
    "print(\"Train set\")\n",
    "for cat in sorted(train_cat_size):\n",
    "    print(\"{:26}: {}\".format(cat, train_cat_size[cat]))\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Valid set\")\n",
    "for cat in sorted(valid_cat_size):\n",
    "    print(\"{:26}: {}\".format(cat, valid_cat_size[cat]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 3806 samples\n",
      "Validation data size: 1019 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data size: {} samples\".format(sum(train_cat_size.values())))\n",
    "print(\"Validation data size: {} samples\".format(sum(valid_cat_size.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV2 manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cv2_preprocess_function(img):\n",
    "    '''Remove background and leave only seedling.\n",
    "    Inputs\n",
    "    ------\n",
    "        img : numpy array, BGR image \n",
    "    Returns\n",
    "    -------\n",
    "        output : numpy array\n",
    "            Result image\n",
    "    '''\n",
    "    #convert to hsv format\n",
    "    image_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_hsv = np.array([150, 0, 0])\n",
    "    upper_hsv = np.array([179, 255, 255])\n",
    "    #make the mask of green colors in hsv format\n",
    "    mask_range = cv2.inRange(image_hsv,lower_hsv, upper_hsv)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "    mask = cv2.morphologyEx(mask_range, cv2.MORPH_CLOSE, kernel)\n",
    "    #remove background \n",
    "    output = cv2.bitwise_and(img, img, mask = mask)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Balance datacet by oversampling with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_SAMPLES = 500\n",
    "NUM_VALID_SAMPLES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get batches from the dirname directory, perform augmentation (rotation, shift, flip, zoom) \n",
    "# and return batch genarator object.\n",
    "def get_batches(dirname, save_to_dir=None, shuffle=True, batch_size=4, class_mode='input',\n",
    "               target_size=(299, 299)):\n",
    "    gen = image.ImageDataGenerator(\n",
    "                        rotation_range=180, \n",
    "                        #width_shift_range=0.1, height_shift_range=0.1, \n",
    "                        horizontal_flip=True, vertical_flip=True,\n",
    "                        #zoom_range=0.2,\n",
    "                        preprocessing_function=cv2_preprocess_function\n",
    "                        )\n",
    "    flow = gen.flow_from_directory(\n",
    "                        dirname,\n",
    "                        save_to_dir=save_to_dir,\n",
    "                        shuffle=False,\n",
    "                        batch_size=batch_size, \n",
    "                        class_mode=class_mode, \n",
    "                        target_size=target_size\n",
    "                        )\n",
    "\n",
    "    return flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 308 images belonging to 1 classes.\n",
      "Found 184 images belonging to 1 classes.\n",
      "Found 523 images belonging to 1 classes.\n",
      "Found 396 images belonging to 1 classes.\n",
      "Found 412 images belonging to 1 classes.\n",
      "Found 380 images belonging to 1 classes.\n",
      "Found 312 images belonging to 1 classes.\n",
      "Found 176 images belonging to 1 classes.\n",
      "Found 210 images belonging to 1 classes.\n",
      "Found 176 images belonging to 1 classes.\n",
      "Found 488 images belonging to 1 classes.\n",
      "Found 229 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "for cat in os.listdir(train_path):\n",
    "    train_cat_dir = os.path.join(train_path, cat)\n",
    "    save_to_dir = os.path.join(train_preproc_path, cat)\n",
    "    if not os.path.exists(save_to_dir):\n",
    "        os.mkdir(save_to_dir)\n",
    "    batch = get_batches(train_cat_dir, save_to_dir=save_to_dir, batch_size=1, class_mode=\"input\")\n",
    "    for i in range(NUM_TRAIN_SAMPLES):\n",
    "        next(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77 images belonging to 1 classes.\n",
      "Found 47 images belonging to 1 classes.\n",
      "Found 131 images belonging to 1 classes.\n",
      "Found 100 images belonging to 1 classes.\n",
      "Found 104 images belonging to 1 classes.\n",
      "Found 95 images belonging to 1 classes.\n",
      "Found 78 images belonging to 1 classes.\n",
      "Found 45 images belonging to 1 classes.\n",
      "Found 53 images belonging to 1 classes.\n",
      "Found 45 images belonging to 1 classes.\n",
      "Found 123 images belonging to 1 classes.\n",
      "Found 58 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "for cat in os.listdir(valid_path):\n",
    "    valid_cat_dir = os.path.join(valid_path, cat)\n",
    "    save_to_dir = os.path.join(valid_preproc_path, cat)\n",
    "    if not os.path.exists(save_to_dir):\n",
    "        os.mkdir(save_to_dir)\n",
    "    batch = get_batches(valid_cat_dir, save_to_dir=save_to_dir, batch_size=1, class_mode=\"input\")\n",
    "    for i in range(NUM_VALID_SAMPLES):\n",
    "        next(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Black-grass               : 500\n",
      "Charlock                  : 500\n",
      "Cleavers                  : 500\n",
      "Common Chickweed          : 500\n",
      "Common wheat              : 500\n",
      "Fat Hen                   : 500\n",
      "Loose Silky-bent          : 500\n",
      "Maize                     : 500\n",
      "Scentless Mayweed         : 500\n",
      "Shepherds Purse           : 500\n",
      "Small-flowered Cranesbill : 500\n",
      "Sugar beet                : 500\n",
      "\n",
      "Valid set\n",
      "Black-grass               : 200\n",
      "Charlock                  : 200\n",
      "Cleavers                  : 200\n",
      "Common Chickweed          : 200\n",
      "Common wheat              : 200\n",
      "Fat Hen                   : 200\n",
      "Loose Silky-bent          : 200\n",
      "Maize                     : 200\n",
      "Scentless Mayweed         : 200\n",
      "Shepherds Purse           : 200\n",
      "Small-flowered Cranesbill : 200\n",
      "Sugar beet                : 200\n"
     ]
    }
   ],
   "source": [
    "train_cat_size = {}\n",
    "valid_cat_size = {}\n",
    "for cat in os.listdir(train_preproc_path):\n",
    "    train_cat_dir = os.path.join(train_preproc_path, cat)\n",
    "    train_cat_size[cat] = len(os.listdir(train_cat_dir))\n",
    "\n",
    "for cat in os.listdir(valid_preproc_path):\n",
    "    valid_cat_dir = os.path.join(valid_preproc_path, cat)\n",
    "    valid_cat_size[cat] = len(os.listdir(valid_cat_dir))\n",
    "\n",
    "print(\"Train set\")\n",
    "for cat in sorted(train_cat_size):\n",
    "    print(\"{:26}: {}\".format(cat, train_cat_size[cat]))\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Valid set\")\n",
    "for cat in sorted(valid_cat_size):\n",
    "    print(\"{:26}: {}\".format(cat, valid_cat_size[cat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
